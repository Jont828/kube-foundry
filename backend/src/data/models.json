{
  "models": [
    {
      "id": "Qwen/Qwen3-0.6B",
      "name": "Qwen3 0.6B",
      "description": "Small, efficient model ideal for development and testing",
      "size": "0.6B",
      "task": "text-generation",
      "contextLength": 32768,
      "supportedEngines": ["vllm", "sglang", "trtllm"],
      "minGpuMemory": "4GB",
      "minGpus": 1
    },
    {
      "id": "Qwen/Qwen2.5-1.5B-Instruct",
      "name": "Qwen2.5 1.5B Instruct",
      "description": "Instruction-tuned model with strong performance",
      "size": "1.5B",
      "task": "chat",
      "contextLength": 32768,
      "supportedEngines": ["vllm", "sglang", "trtllm"],
      "minGpuMemory": "6GB",
      "minGpus": 1
    },
    {
      "id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "name": "DeepSeek R1 Distill 8B",
      "description": "Reasoning-focused model with strong analytical capabilities",
      "size": "8B",
      "task": "chat",
      "contextLength": 16384,
      "supportedEngines": ["vllm", "sglang"],
      "minGpuMemory": "16GB",
      "minGpus": 1
    },
    {
      "id": "meta-llama/Llama-3.2-1B-Instruct",
      "name": "Llama 3.2 1B Instruct",
      "description": "Compact Llama model optimized for instruction following",
      "size": "1B",
      "task": "chat",
      "contextLength": 131072,
      "supportedEngines": ["vllm", "sglang", "trtllm"],
      "minGpuMemory": "4GB",
      "minGpus": 1,
      "gated": true
    },
    {
      "id": "meta-llama/Llama-3.2-3B-Instruct",
      "name": "Llama 3.2 3B Instruct",
      "description": "Balanced Llama model for various tasks",
      "size": "3B",
      "task": "chat",
      "contextLength": 131072,
      "supportedEngines": ["vllm", "sglang", "trtllm"],
      "minGpuMemory": "8GB",
      "minGpus": 1,
      "gated": true
    },
    {
      "id": "mistralai/Mistral-7B-Instruct-v0.3",
      "name": "Mistral 7B Instruct v0.3",
      "description": "Powerful instruction-tuned model from Mistral AI",
      "size": "7B",
      "task": "chat",
      "contextLength": 32768,
      "supportedEngines": ["vllm", "sglang", "trtllm"],
      "minGpuMemory": "16GB",
      "minGpus": 1,
      "gated": true
    },
    {
      "id": "microsoft/Phi-3-mini-4k-instruct",
      "name": "Phi-3 Mini 4K Instruct",
      "description": "Microsoft's efficient small language model",
      "size": "3.8B",
      "task": "chat",
      "contextLength": 4096,
      "supportedEngines": ["vllm", "sglang"],
      "minGpuMemory": "8GB",
      "minGpus": 1
    },
    {
      "id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "name": "TinyLlama 1.1B Chat",
      "description": "Lightweight chat model for resource-constrained environments",
      "size": "1.1B",
      "task": "chat",
      "contextLength": 2048,
      "supportedEngines": ["vllm", "sglang", "trtllm"],
      "minGpuMemory": "4GB",
      "minGpus": 1
    },
    {
      "id": "kaito/llama3.2-1b",
      "name": "Llama 3.2 1B (GGUF)",
      "description": "Compact GGUF model for CPU inference - edge deployments",
      "size": "1B",
      "task": "chat",
      "supportedEngines": ["llamacpp"],
      "license": "Llama",
      "minGpus": 0
    },
    {
      "id": "kaito/llama3.2-3b",
      "name": "Llama 3.2 3B (GGUF)",
      "description": "Efficient GGUF model for CPU inference - general tasks",
      "size": "3B",
      "task": "chat",
      "supportedEngines": ["llamacpp"],
      "license": "Llama",
      "minGpus": 0
    },
    {
      "id": "kaito/llama3.1-8b",
      "name": "Llama 3.1 8B (GGUF)",
      "description": "Balanced GGUF model for CPU inference - performance and efficiency",
      "size": "8B",
      "task": "chat",
      "supportedEngines": ["llamacpp"],
      "license": "Llama",
      "minGpus": 0
    },
    {
      "id": "kaito/llama3.3-70b",
      "name": "Llama 3.3 70B (GGUF)",
      "description": "High-performance GGUF model - requires significant memory",
      "size": "70B",
      "task": "chat",
      "supportedEngines": ["llamacpp"],
      "license": "Llama",
      "minGpus": 0
    },
    {
      "id": "kaito/mixtral-8x7b",
      "name": "Mixtral 8x7B (GGUF)",
      "description": "Mixture of experts GGUF model for CPU inference",
      "size": "8x7B",
      "task": "chat",
      "supportedEngines": ["llamacpp"],
      "license": "Apache",
      "minGpus": 0
    },
    {
      "id": "kaito/phi4-14b",
      "name": "Phi 4 14B (GGUF)",
      "description": "Microsoft research model in GGUF format for CPU inference",
      "size": "14B",
      "task": "chat",
      "supportedEngines": ["llamacpp"],
      "license": "MIT",
      "minGpus": 0
    },
    {
      "id": "kaito/gemma2-2b",
      "name": "Gemma 2 2B (GGUF)",
      "description": "Google lightweight GGUF model for CPU inference",
      "size": "2B",
      "task": "chat",
      "supportedEngines": ["llamacpp"],
      "license": "Gemma",
      "minGpus": 0
    },
    {
      "id": "kaito/qwq-32b",
      "name": "QwQ 32B (GGUF)",
      "description": "Reasoning-focused GGUF model for CPU inference",
      "size": "32B",
      "task": "chat",
      "supportedEngines": ["llamacpp"],
      "license": "Apache 2.0",
      "minGpus": 0
    },
    {
      "id": "kaito/codestral-22b",
      "name": "Codestral 22B (GGUF)",
      "description": "Code generation specialist in GGUF format",
      "size": "22B",
      "task": "text-generation",
      "supportedEngines": ["llamacpp"],
      "license": "MNLP",
      "minGpus": 0
    },
    {
      "id": "kaito/gpt-oss-20b",
      "name": "GPT-OSS 20B (GGUF)",
      "description": "Open source GPT-style GGUF model for CPU inference",
      "size": "20B",
      "task": "chat",
      "supportedEngines": ["llamacpp"],
      "license": "Apache 2.0",
      "minGpus": 0
    }
  ]
}
